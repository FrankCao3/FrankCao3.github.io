<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>机器学习实战 | CCB</title><meta name="keywords" content="机器学习"><meta name="author" content="ccb"><meta name="copyright" content="ccb"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="机器学习实战"><meta name="application-name" content="机器学习实战"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="机器学习实战"><meta property="og:url" content="https://frankcao3.github.io/posts/61497.html"><meta property="og:site_name" content="CCB"><meta property="og:description" content="机器学习实战参考书目： 《机器学习实战：基于Scikit-Learn和TensorFlow》 《机器学习实战》 先导知识： 监督学习中，一般使用两种类型的目标变量，标称型和数值型。 标称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{ 爬行类、鱼类、哺乳类、两栖类、植物、真菌}；数值型"><meta property="og:locale" content="en"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="ccb"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="机器学习实战参考书目： 《机器学习实战：基于Scikit-Learn和TensorFlow》 《机器学习实战》 先导知识： 监督学习中，一般使用两种类型的目标变量，标称型和数值型。 标称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{ 爬行类、鱼类、哺乳类、两栖类、植物、真菌}；数值型"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://frankcao3.github.io/posts/61497"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"copy":true,"copyrightEbable":true,"limitCount":50,"languages":{"author":"Author: ccb","link":"Link: ","source":"Source: CCB","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.","copySuccess":"Copy success, copy and reprint please mark the address of this article"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'CCB',
  title: '机器学习实战',
  postAI: '',
  pageFillDescription: '机器学习实战, 先导知识：, 第一章 线性模型, 1. 线性回归（回归）, 2. 梯度下降, 3. 多项式回归, 4. 学习曲线, 5. 正则线性模型, 6. 逻辑回归（分类）, 第二章 KNN算法, 第三章 SVM算法, 1. 线性SVM分类, 2. 非线性SVM分类, 3. SVM回归(SVR), 第四章 决策树DT, 一、CART算法, 1. CART算法（分类）, 2. CART算法（回归）, 3. 优缺点, 二、ID3算法, 三、C4.5算法（J48）, 第五章 集成学习与随机森林, 一、模型融合, 1. 投票分类器, 2. bagging 和 pasting, 3. stacking, 二、随机森林RF, 三、提升法boosting, 1. AdaBoost, 2. 梯度提升, 四、XGBoost, 第六章 贝叶斯分类器, 一、贝叶斯公式：, 二、朴素贝叶斯模型：, 1. 多项式模型：, 2. 高斯模型：, 3. 伯努利模型：, 第七章 神经网络, 第八章 聚类, 附录：机器学习实战参考书目机器学习实战基于和机器学习实战先导知识监督学习中一般使用两种类型的目标变量标称型和数值型标称型目标变量的结果只在有限目标集中取值如真与假动物分类集合爬行类鱼类哺乳类两栖类植物真菌数值型目标变量则可以从无限的数值集合中取值如等标称型目标变量主要用来分类数值型目标变量主要用于回归分析机器学习的主要任务分类和回归分类是将实例数据划分到合适的分类中回归主要用于预测数值型数据分类和回归均属于监督学习即这类算法必须知道样本的类别即目标变量的分类信息另外无监督学习则指训练数据没有类别信息也不会给定目标值无监督学习可以进行聚类密度估计寻找描述数据统计值也可以减少数据特征的维度以便使用二维或三维图形来更直观地展示数据信息如何选择算法首先考虑使用算法的目的如果是想要知道数据的目标变量的值可以选择监督学习算法如果目标变量是离散的就选择分类器算法如果目标变量是连续的就选择回归算法如果不想预测目标变量的值可以选择无监督学习算法如果是要将数据划分为离散的组就选择聚类算法如果需要估计数据与每个分组的相似程度则需要使用密度估计算法其次需要考虑数据问题特征值是离散型变量还是连续型特征值中是否存在缺失的值何种原因造成的缺失数据中是否存在异常值某个特征发生的频率如何等等开发机器学习应用程序的步骤收集数据准备输入数据分析输入数据查看数据中是否存在空值异常值等确保没有垃圾数据训练算法测试算法使用算法第一章线性模型线性回归回归线性模型就是对输入特征加权求和再加上一个我们称为偏置项也称为截距项的常数以此进行预测由于线性回归模型的梯度下降对特征缩放敏感所以需要保证所有特征值的大小比例都差不多比如使用的类否则收敛的时间会长很多存放回归系数则存放截距其实大多数情况下不会使用纯线性回归容易过拟合通常会加上正则化后面会讲到计算复杂度特征数量比较大例如时线性回归的计算将极其缓慢但是另一方面对于训练集中的实例数量来说需要计算的方程是线性的所以能够有效地处理大量的训练集只要内存足够在预测方面线性回归模型一经训练不论是标准方程还是梯度下降等其他算法预测就非常快速因为计算复杂度相对于想要预测的实例数量和特征数量来说都是线性的梯度下降标准方程求线性模型的最优参数可以通过标准方程直接计算得到但是涉及计算样本特征矩阵的内积计算复杂度较高但是优点是不需要特征缩放不需要选择学习率以及进行迭代梯度下降梯度下降是一种非常通用的优化算法能够为大范围的问题找到最优解梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化线性回归模型的成本函数恰好是个凸函数所以在梯度下降时不会陷入局部最优应用梯度下降时需要保证所有特征值的大小比例都差不多比如使用的类否则收敛的时间会长很多批量梯度下降基于整个训练数据集对所有需要优化的参数进行同步优化因此面对非常庞大的训练集时算法会变得极慢不过我们即将看到快得多的梯度下降算法但是梯度下降算法随特征数量扩展的表现比较好如果要训练的线性模型拥有几十万个特征使用梯度下降比标准方程要快得多学习率控制了梯度下降的快慢学习率太低算法消耗时间太久学习率太高难以稳定到最优参数要找到合适的学习率可以使用网格搜索另外参数优化的迭代次数也要适中太低可能还未达到最优太高浪费时间一个简单的办法是在开始时设置一个非常大的迭代次数但是当梯度向量的值变得很微小时中断算法也就是当它的范数变得低于称为容差时因为这时梯度下降已经几乎到达了最小值随机梯度下降批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度所以训练集很大时算法会特别慢与之相反的极端是随机梯度下降每一步在训练集中随机选择一个实例并且仅基于该单个实例来计算梯度这让算法变得快多了也可以被用来训练海量的数据集另一方面由于算法的随机性质它比批量梯度下降要不规则得多成本函数将不再是缓缓降低直到抵达最小值而是不断上上下下但是从整体来看还是在慢慢下降但是即使它到达了最小值依旧还会持续反弹永远不会停止所以算法停下来的参数值肯定是足够好的但不是最优的随机梯度下降其实可以帮助算法跳出局部最小值所以相比批量梯度下降它对找到全局最小值更有优势因此随机性的好处在于可以逃离局部最优但缺点是永远定位不出最小值要解决这个困境有一个办法是逐步降低学习率模拟退火随机梯度下降为迭代次数为惩罚项正则化为迭代次数小批量梯度下降每一步的梯度计算基于一小部分随机的实例集也就是小批量相比随机梯度下降小批量梯度下降的主要优势在于可以从矩阵运算的硬件优化中获得显著的性能提升特别是需要用到图形处理器时这个算法在参数空间层面的前进过程也不像那样不稳定特别是批量较大时所以小批量梯度下降最终会比更接近最小值一些但是另一方面它可能更难从局部最小值中逃脱多项式回归当数据集并非线性时可以将每个特征的幂次方添加为一个新特征在拓展过的特征集上训练线性模型这种方法被称为多项式回归会在给定的多项式阶数下添加所有特征组合例如有两个特征和阶数不只会添加特征和还会添加组合以及所以要小心特征组合的数量爆炸多项式回归增加原特征的平方作为新特征现在包含原本的特征和该特征的平方或者使用管道技术一次完成学习曲线模型训练时会出现欠拟合和过拟合的问题如果无论怎么增大数据集训练集和测试集的误差接近且较高那么模型可能欠拟合这时增大数据集是无效的需要使用更复杂的模型和寻找更好的特征如果训练集和测试集的误差有一定差距且在训练集上模型的表现好很多那么说明模型过拟合了这时需要增加训练数据直到测试误差接近训练误差或者对模型进行正则化正则线性模型减少过度拟合的一个好办法就是对模型正则化即约束它它拥有的自由度越低就越不容易过度拟合数据比如对于多项式模型来说正则化可以是降低多项式的阶数对于线性模型正则化通常通过约束模型的权重来实现岭回归在成本函数中添加一个正则项使得学习中的算法不仅需要拟合数据同时还要让模型权重保持最小在执行岭回归之前必须对数据进行缩放例如使用因为它对输入特征的大小非常敏感大多数正则化模型都是如此超参数控制的是对模型进行正则化的程度如果则岭回归就是线性模型如果非常大那么所有的权重都将非常接近于零结果是一条穿过数据平均值的水平线注意正则项只能在训练的时候添加到成本函数中一旦训练完成你需要使用未经正则化的性能指标来评估模型性能使用标准方程求解的岭回归使用梯度下降的岭回归正则项为表示岭回归超参数设置的是使用正则项的类型设为表示希望在成本函数中添加一个正则项等于权重向量的范数的平方的一半即岭回归套索回归与岭回归一样它也是向成本函数增加一个正则项但是它增加的是权重向量的范数而不是范数的平方的一半回归的一个重要特点是它倾向于完全消除掉最不重要特征的权重也就是将它们设置为零也就是说回归会自动执行特征选择并输出一个稀疏模型即只有很少的特征有非零权重使用回归弹性网络弹性网络是岭回归与回归之间的中间地带其正则项就是岭回归和回归的正则项的混合混合比例通过来控制当时弹性网络即等同于岭回归而当时即相当于回归使用弹性网络如何选择正则化模型模型正则化比不正则化可取大多数情况下应该避免使用纯线性回归岭回归是个不错的默认选择但是如果你觉得实际用到的特征只有少数几个那就应该更倾向于回归或是弹性网络因为它们会将无用特征的权重降为零一般而言弹性网络优于回归因为当特征数量超过训练实例数量又或者是几个特征强相关时回归的表现可能非常不稳定逻辑回归分类将数据正则化之后比如使用类使用类进行模型建立逻辑回归被广泛用于估算一个实例属于某个特定类别的概率如果你要求它预测出类别使用方法而不是方法它将返回一个可能性最大的类别与其他线性模型一样逻辑回归模型可以用或惩罚函数来正则化默认添加的是函数控制的模型正则化程度的超参数不是其他线性模型使用而是它的逆反的值越高模型正则化程度越高回归逻辑回归模型经过推广可以直接支持多个类别而不需要训练并组合多个二元分类器如第章所述这就是回归原理很简单对于一个给定的实例回归模型首先计算出每个类别的分数然后对这些分数应用函数也叫归一化指数估算出每个类别的概率跟逻辑回归分类器一样回归分类器将估算概率值最高的类别作为预测类别也就是分数最高的类别回归分类器一次只会预测一个类别也就是说它是多类别但是不是多输出所以它应该仅适用于互斥的类别之上回归的训练目标是得到一个能对目标类别做出高概率估算的模型也就是其他类别的概率相应要很低其成本函数交叉熵如下当只有两个类别时该成本函数等价于逻辑回归的成本函数损失函数第二章算法优点精度高对异常值不敏感无数据输入假定缺点计算复杂度高空间复杂度高适用范围数值型和标称型对未知类别属性的数据集中的每个点依次执行以下操作计算已知类别数据集中的点与当前点之间的距离欧氏距离按照距离递增次序排序选取与当前点距离最小的个点确定前个点所在类别的出现频率返回前个点出现频率最高的类别作为当前点的预测分类折交叉验证当然也可以用来处理回归任务返回邻近的个样本点的标签值的平均数作为预测值处理回归任务第三章算法能够执行线性或非线性分类回归甚至是异常值检测任务它是机器学习领域最受欢迎的模型之一特别适用于中小型复杂数据集的分类优点用于二元和多元分类器回归和新奇性检测良好的预测生成器提供了鲁棒的过拟合噪声数据和异常点处理当变量比样本还多是依旧有效快速即使样本量大于万自动检测数据的非线性不用做变量变换缺点应用在二元分类表现最好其他预测问题表现不是太好变量比样例多很多的时候有效性降低需要使用其他方案例如方案只提供预测结果如果想要获取预测概率需要额外方法去获取如果想要最优结果需要调参使用预测模型的通用步骤选择使用的类用数据训练模型检查验证误差并作为基准线为参数尝试不同的值检查验证误差是否改进再次使用最优参数的数据来训练模型线性分类生成决策边界实线所示不仅分离类别并且尽可能远离最近的训练实例大间隔分类决策边界由最接近边界的训练实例确定支持这些实例被称为支持向量下图中已圈出可以将分类器视为在类别之间拟合可能的最宽的街道平行的虚线所示对特征缩放非常敏感在垂直刻度和水平刻度上生成的决策边界可能存在很大的差异在左图中垂直刻度比水平刻度大得多因此可能的决策边界接近于水平在特征缩放例如使用的后决策边界看起来好很多见右图软间隔分类如果严格地让所有实例都不在街道上并且位于正确的一边这就是硬间隔分类硬间隔分类有个问题一个是它只在数据线性可分离时才有效一个是对异常值特别敏感会影响泛化能力为了避免以上问题灵活地使用模型我们尽可能在保持街道宽阔和限制间隔违例即位于街道之上甚至在错误的一边的实例之间找到良好的平衡这就是软间隔分类在的类中可以通过超参数来控制这个平衡值越小则街道越宽但是间隔违例也会越多如果你的模型过度拟合可以试试通过降低来进行正则化算法在求解的决策边界求解参数时会遇到规模正比于训练样本数量的问题为了避开这个障碍使用算法来求解其中的参数的思想每次选择个变量然后固定其他变量参数然后优化选择的这个变量因为每次只优化个变量所以非常高效使用类适用于样本数量较多的二元和多元分类大于它会对偏置项进行正则化所以你需要先减去平均值使训练集集中归一化如果使用会自动进行这一步此外请确保超参数设置为因为它不是默认值最后为了获得更好的性能还应该将超参数设置为除非特征数量比训练实例还多可以使用管道技术将归一化和实例化算法统一起来越大越接近硬间隔使用损失函数随机种子为类可以使用核函数后面会讲到适用于样本数量较少的二元和多元分类少于使用线性核函数类它不会像类那样快速收敛但是对于内存处理不了的大型数据集核外训练或是在线分类任务它非常有效模型使用损失函数为正则化项参数非线性分类有些情况下数据集无法直接做到线性可分解决方法之一就是添加更多的特征将原始数据映射到更高维的空间使其变得线性可分核函数添加特征会使得在高维空间计算样本特征内积变得困难为了避开这个障碍可以设想一个函数使样本在高维特征空间的内积等于它们在原始样本空间中通过该函数计算的结果使用该函数的就可以替代在高维甚至无穷维特征空间中的内积这个函数就叫做核函数线性核模型使用线性核函数多项式核为了将数据集映射到高维特征空间可以使用转换器直接添加多项式特征但问题是如果多项式太低阶处理不了非常复杂的数据集而高阶则会创造出大量的特征导致模型变得太慢为了解决这个问题出现了多项式核函数下面这段代码使用了一个阶多项式内核训练分类器超参数控制的是模型受高阶多项式还是低阶多项式影响的程度高斯核使用高斯核函数另外还有拉普拉斯核核如何选择核函数有一个经验法则是永远先从线性核函数开始尝试要记住比快得多特别是训练集非常大或特征非常多的时候如果训练集不太大你可以试试高斯核大多数情况下它都非常好用如果你还有多余的时间和计算能力你可以使用交叉验证和网格搜索来尝试一些其他的核函数特别是那些专门针对你的数据集数据结构的核函数回归算法非常全面它不仅支持线性和非线性分类而且还支持线性和非线性回归回归要做的是让尽可能多的实例位于街道上同时限制间隔违例也就是不在街道上的实例街道的宽度由超参数控制在间隔内添加更多的实例不会影响模型的预测所以这个模型被称为不敏感回归要解决非线性回归任务可以使用核化的模型使用核化的模型进行回归第四章决策树决策树是一种由结点和有向边构成的树形结构结点类型分为内部结点和叶结点每个内部结点代表对象的一个特征叶结点则代表对象的类别决策树中每一个深度就是一次根据某一特征做出的判断决策树的特质之一就是它们需要的数据准备工作非常少特别是完全不需要进行特征缩放或集中鸢尾花决策树节点的属性统计它应用的训练实例数量满足该节点属性的实例数量属性说明了该节点上每个类别的训练实例数量属性衡量其不纯度基尼不纯度如果应用的所有训练实例都属于同一个类别那么节点就是纯的是第个节点上类别为的训练实例占比使用的是算法该算法仅生成二叉树可用于分类和回归使用基尼不纯度来划分属性但是其他算法比如生成的决策树其节点可以拥有两个以上的子节点使用信息增益来划分属性一算法算法分类过程如下使用单个特征和阈值例如花瓣长度厘米将训练集分成两个子集和就是使得成本函数最小化或者信息增益最大化的成本函数衡量划分后的子集的不纯度一旦成功将训练集一分为二它将使用相同的逻辑继续分裂子集然后是子集的子集依次循环递进抵达最大深度由超参数控制或是再也找不到能够降低不纯度的分裂时停止明显决策树的思想是一种贪心选择它并不会检视一次分裂的不纯度是否为可能的最低值这样通常可以产生一个相当不错的解但是不能保证是最优解而寻找最优树是一个完全问题即使训练集很小时间复杂度也很高很难解决所以我们必须接受这个相当不错的解计算复杂度决策树总体预测复杂度是为实例数量可以看出复杂度与特征数量无关所以即便是处理大型数据集预测也很快但是训练时在每一个节点算法都需要在所有样本上比较所有特征如果设置了划分时考虑的最大特征数会少一些这导致训练的复杂度为对于小型训练集几千个实例以内可以通过对数据预处理设置表示对样本进行预排序来加快训练但是对于较大训练集而言可能会减慢训练的速度过拟合与正则化超参数决策树在训练时不会确定参数的数量树的深度不确定也叫非参数模型这会导致模型结构自由而紧密地贴近数据很可能过拟合而比如线性回归有预先设定好一部分参数所以其自由度受限降低过拟合的风险但是相应的也增加了拟合不足的风险为了避免过拟合需要在训练中降低决策树的自由度即正则化正则化超参数的选择取决于使用的模型但是通常至少可以限制决策树的深度在中这由超参数控制默认值为意味着无限制减小可使模型正则化从而降低过度拟合的风险另外类还有一些其他的参数也可以限制决策树的形状比如分裂前节点必须有的最小样本数叶节点必须有的最小样本数量等还可以先不加约束地训练模型然后再对不必要的节点进行剪枝删除比如一个节点的子节点全部为叶节点则该节点可被认为不必要删除比如测试是用来估算提升纯粹是出于偶然被称为虚假设的概率如果这个概率称之为值高于一个给定阈值通常是由超参数控制那么这个节点可被认为不必要其子节点可被删除算法回归决策树也可以用来完成回归任务用的来构建一个回归树与分类决策树的主要差别在于每个节点上不再是预测一个类别而是预测一个值预测结果就是与最后到达的叶节点关联的个实例的平均目标值表示在这个叶节点上得到的预测结果的均方误差回归任务中算法的工作原理跟前面介绍的大致相同唯一不同在于它分裂训练集的方式不是最小化不纯度而是最小化同样用于回归的决策树也会有过拟合的可能所以需要设置优缺点决策树使用简单不受特征数量的限制但是青睐正交的决策边界所有的分裂都与轴线垂直这导致它们对训练集的旋转非常敏感可能导致泛化不佳限制这种问题的方法之一是使用让训练数据定位在一个更好的方向上更概括地说决策树的主要问题是它们对训练数据中的小变化非常敏感二算法三算法使用信息增益选择特征第五章集成学习与随机森林一模型融合投票分类器不同的算法在相同训练集上进行训练得到多个预测模型然后基于多个模型的预测结果投票选出最终结果硬投票法直接让各个预测模型给出预测结果投票然后选择大多数模型投票的类别作为最终预测结果软投票法模型估算出类别的概率将概率在所有单个分类器上加权平均选出平均概率最高的类别进行预测软投票法比硬投票的表现更优因为它基于哪些高度自信的投票更高的权重和对训练集随机采样使用相同的算法在不同的训练子集上进行训练采样时如果将样本放回这种方法叫不放回叫表示否则表示一旦预测器训练完成集成就可以通过简单地聚合所有预测器的预测来对新实例做出预测聚合函数通常是统计法即最多数的预测好比硬投票分类器一样用于分类或是平均法用于回归最终结果是与直接在原始训练集上训练的单个预测器相比集成方法的单个预测器的偏差更大但是最终结果的偏差相近方差更低包外评估使用时有些样本可能会被多次采样有些样本可能不会被采样不划分单独的测试集而直接将那些未被采样的样本作为测试集就是包外评估第一层将数据分为训练集和测试集训练集再分为个子集首先在子集上训练不同的模型第二层分别使用前面训练好的几个模型对子集进行预测得到多个预测值接着使用这些预测值作为输入特征创建一个新的训练集并保留真实标签在这个新的训练集上训练混合器让它学习根据第一层的预测来在测试集上预测目标值当然这个模型还可以增加层数增加的层都是使用上一层的预测值作为输入特征来训练模型比如下面是一个三层的模型将训练集分为个子集第一层使用子集进行模型的训练得到个模型第二层中使用第一层的模型在子集上的预测作为输入特征来训练三个新的模型第三层中使用第二层的模型在子集上的预测作为输入特征来训练最终模型最后使用最终模型在测试集上进行预测不幸的是不直接支持堆叠但是自己堆出的实现并不太难或者也可以使用开源的实现方案例如可从获得二随机森林随机森林就是决策树的集成通常采用集成方法有时也可以是训练子集的大小通过来设置在中除了先构建一个然后将结果传输到还有一种方法就是直接使用或者类随机森林在树的生长上引入了更多的随机性分裂节点时不再是搜索最好的特征而是在一个随机生成的特征子集里搜索最好的特征这导致决策树具有更大的多样性用更高的偏差换取更低的方差总之还是产生了一个整体性能更优的模型三提升法提升法最初被称为假设提升是指可以将几个弱学习器结合成一个强学习器的任意集成方法大多数提升法的总体思路是循环训练预测器每一次都对其前序做出一些改正新预测器对其前序进行纠正的办法之一就是更多地关注前序拟合不足的训练实例从而使新的预测器不断地越来越专注于难缠的问题这就是使用的技术过程如下训练一个基础分类器比如决策树用它对训练集进行预测然后对错误分类的训练实例增加其相对权重使用这个最新的权重对第二个分类器进行训练然后再次对训练集进行预测继续对错误分类的训练实例增加其权重如此循环当到达所需数量的预测器或得到完美的预测器时算法停止这样就得到若干个预测器再使用或等集成方法得到最终预测结果优缺点不再是调整单个预测器的参数使损失函数最小化而是不断在集成中加入预测器使模型越来越好而且每次训练是基于加权的训练集这种依序学习技术有一个重要的缺陷就是无法并行哪怕只是一部分因为每个预测器只能在前一个预测器训练完成并评估之后才能开始训练在这一点上的表现不及和方法权重中每个预测器有一个权重通过其加权误差率学习率计算而来预测器的准确率越高其权重就越高同时每个样本实例也有权重最开始每个实例的权重都一样一个预测器预测完成后会对实例的权重进行更新也就是提升被错误分类的实例的权重使用使用的其实是的一个多分类版本叫作基于多类指数损失函数的逐步添加模型当只有两个类别时即等同于此外如果预测器可以估算类别概率即具有方法会使用一种的变体称为代表它依赖的是类别概率而不是类别预测通常表现更好梯度提升四第六章贝叶斯分类器贝叶斯分类是一类分类算法的总称这类算法均以贝叶斯定理为基础故统称为贝叶斯分类而朴素朴素贝叶斯分类是贝叶斯分类中最简单也是常见的一种分类方法一贝叶斯公式原理特征向量类别先验概率指根据以往经验和分析得到的概率后验概率事情已经发生要求这件事情发生的原因是由某个因素引起的可能性的大小类条件概率在已知某类别的特征空间中出现特征值的概率密度如果需要选出某样本属于哪类则需要根据该条样本求出它属于每个类的概率选择最大概率的那个类作为分类结果由于结果的产生是比较属于各个类别的概率所以计算的概率的分母都是可以忽略掉同时容易求出那么我们关注朴素贝叶斯之所以朴素是因为它假设的每个特征都是独立的回归原始故而的概率就可以计算为故朴素贝叶斯公式二朴素贝叶斯模型多项式模型多项式模型在计算先验概率和条件概率时会做一些平滑处理具体公式为类别为的样本数总样本数总的类别个数平滑值类别为且特征为的样本数特征可以选择的数量多项式朴素贝叶斯为平滑参数默认为为类先验概率若指定了该参数就按指定的参数计算取值就是转换成后的结果防止下溢出默认为表示是否学习先验概率为时表示所有类标记具有相同的先验概率等于类标记总个数分之一高斯模型当特征是连续变量的时候假设特征分布为正太分布根据样本算出均值和方差再求得概率高斯朴素贝叶斯参数默认为指各个类标记对应的先验概率表示增量学习伯努利模型伯努利模型适用于离散特征的情况每个特征的取值只能是和伯努利朴素贝叶斯每个特征的取值只能是和参数指将数据特征二值化的阈值算法流程处理数据得到个具有个特征的样本这些样本分别属于类别通过数据分析可以得到每个特征的类条件概率再通过全概率公式求得其中可根据特征独立性展开将求得的先验概率和类条件概率带入朴素贝叶斯公式求得每个类别的后验概率我们可以选择概率最大的类别为最后确定的类别第七章神经网络第八章聚类附录机器学习实战目录为分类算法为回归算法为无监督算法基础概率分布算法优化处理数据集合中的缺失值回归去噪局部线性回归回归部分主成分分析奇异值分解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-06 17:09:05',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="CCB" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">CCB</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> Newest Comments</span></div><div class="aside-list"><span>loading...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/DVWA%E9%9D%B6%E5%9C%BA/" style="font-size: 1.05rem;">DVWA靶场<sup>10</sup></a><a href="/tags/%E5%AE%9E%E8%AE%AD-%E7%BD%91%E7%BB%9C%E5%BB%BA%E8%AE%BE%E9%83%A8%E5%88%86/" style="font-size: 1.05rem;">实训-网络建设部分<sup>7</sup></a><a href="/tags/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">搭建博客<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">机器学习<sup>1</sup></a><a href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" style="font-size: 1.05rem;">渗透测试<sup>15</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 1.05rem;">计算机基础<sup>5</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">August 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">15</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/07/"><span class="card-archive-list-date">July 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/08/"><span class="card-archive-list-date">August 2021</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2020/08/"><span class="card-archive-list-date">August 2020</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">18</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>机器学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">机器学习实战</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-07-30T04:20:29.000Z" title="Created 2022-07-30 12:20:29">2022-07-30</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-10-06T09:09:05.971Z" title="Updated 2023-10-06 17:09:05">2023-10-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://frankcao3.github.io/posts/61497"><header><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">机器学习</a><h1 id="CrawlerTitle" itemprop="name headline">机器学习实战</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">ccb</span><time itemprop="dateCreated datePublished" datetime="2022-07-30T04:20:29.000Z" title="Created 2022-07-30 12:20:29">2022-07-30</time><time itemprop="dateCreated datePublished" datetime="2023-10-06T09:09:05.971Z" title="Updated 2023-10-06 17:09:05">2023-10-06</time></header><h1 id="机器学习实战"><a href="#机器学习实战" class="headerlink" title="机器学习实战"></a>机器学习实战</h1><p>参考书目：</p>
<p>《机器学习实战：基于Scikit-Learn和TensorFlow》</p>
<p>《机器学习实战》</p>
<h2 id="先导知识："><a href="#先导知识：" class="headerlink" title="先导知识："></a>先导知识：</h2><ol>
<li><p><strong>监督学习中，一般使用两种类型的目标变量，标称型和数值型。</strong></p>
<p>标称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{ 爬行类、鱼类、哺乳类、两栖类、植物、真菌}；数值型目标变量则可以从无限的数值集合中取值，如0.100、42.001、1000.743 等。标称型目标变量主要用来分类，数值型目标变量主要用于回归分析。</p>
</li>
<li><p><strong>机器学习的主要任务：分类和回归。</strong></p>
<p>分类是将实例数据划分到合适的分类中，回归主要用于预测数值型数据。分类和回归均属于<strong>监督学习</strong>，即这类算法必须知道样本的类别，即目标变量的分类信息。另外，<strong>无监督学习</strong>则指训练数据没有类别信息，也不会给定目标值。无监督学习可以进行聚类、密度估计（寻找描述数据统计值），也可以减少数据特征的维度，以便使用二维或三维图形来更直观地展示数据信息。</p>
</li>
<li><p><strong>如何选择算法：</strong></p>
<p><em>（1）首先考虑使用算法的目的。</em></p>
<p>如果是想要知道数据的目标变量的值，可以选择监督学习算法。如果目标变量是离散的，就选择分类器算法；如果目标变量是连续的，就选择回归算法。</p>
<p>如果不想预测目标变量的值，可以选择无监督学习算法。如果是要将数据划分为离散的组，就选择聚类算法；如果需要估计数据与每个分组的相似程度，则需要使用密度估计算法。</p>
<p><em>（2）其次需要考虑数据问题。</em></p>
<p>特征值是离散型变量还是连续型，特征值中是否存在缺失的值，何种原因造成的缺失，数据中是否存在异常值，某个特征发生的频率如何等等。</p>
</li>
<li><p><strong>开发机器学习应用程序的步骤：</strong></p>
<p>（1）收集数据；</p>
<p>（2）准备输入数据；</p>
<p>（3）分析输入数据：查看数据中是否存在空值、异常值等，确保没有垃圾数据。</p>
<p>（4）训练算法；</p>
<p>（5）测试算法；</p>
<p>（6）使用算法。</p>
</li>
</ol>
<h2 id="第一章-线性模型"><a href="#第一章-线性模型" class="headerlink" title="第一章 线性模型"></a>第一章 线性模型</h2><h4 id="1-线性回归（回归）"><a href="#1-线性回归（回归）" class="headerlink" title="1. 线性回归（回归）"></a>1. 线性回归（回归）</h4><p>线性模型就是对输入特征加权求和，再加上一个我们称为偏置项（也称为截距项）的常数，以此进行预测。</p>
<p>由于线性回归模型的梯度下降对特征缩放敏感，所以需要保证所有特征值的大小比例都差不多（比如使用Scikit-Learn的StandardScaler类），否则收敛的时间会长很多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line"><span class="comment"># coef_存放回归系数，intercept_则存放截距</span></span><br><span class="line"><span class="built_in">print</span>(lin_reg.intercept_, lin_reg.coef_)</span><br><span class="line"><span class="built_in">print</span>(lin_reg.predict(X_new))</span><br></pre></td></tr></table></figure>

<p>其实大多数情况下不会使用纯线性回归，容易过拟合。通常会加上正则化（后面会讲到）。</p>
<p><u><em><strong>计算复杂度：</strong></em></u></p>
<p>特征数量比较大（例如100000）时，线性回归的计算将极其缓慢。但是另一方面，对于训练集中的实例数量来说，需要计算的方程是线性的，所以能够有效地处理大量的训练集，只要内存足够。在预测方面，线性回归模型一经训练（不论是标准方程还是梯度下降等其他算法），预测就非常快速：因为计算复杂度相对于想要预测的实例数量和特征数量来说，都是线性的。</p>
<h4 id="2-梯度下降"><a href="#2-梯度下降" class="headerlink" title="2. 梯度下降"></a>2. 梯度下降</h4><p><u><em><strong>标准方程：</strong></em></u></p>
<p>求线性模型的最优参数可以通过标准方程直接计算得到，但是涉及计算样本特征矩阵的内积，计算复杂度较高。但是优点是不需要特征缩放，不需要选择学习率以及进行迭代。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427104257714.png" alt="image-20210427104257714" style="zoom:50%;">



<p><u><em><strong>梯度下降：</strong></em></u></p>
<p>梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。</p>
<p>线性回归模型的成本函数恰好是个凸函数，所以在梯度下降时不会陷入局部最优。</p>
<p>应用梯度下降时，需要保证所有特征值的大小比例都差不多（比如使用Scikit-Learn的StandardScaler类），否则收敛的时间会长很多。</p>
<p><em><strong>批量梯度下降（BGD）：</strong></em></p>
<p>基于整个训练数据集对所有需要优化的参数进行同步优化。因此，<u>面对非常庞大的训练集时，算法会变得极慢</u>（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法<u>随特征数量扩展的表现比较好</u>：如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程要快得多。</p>
<p><strong>学习率</strong>控制了梯度下降的快慢，学习率太低，算法消耗时间太久，学习率太高，难以稳定到最优参数。要找到合适的学习率，可以使用网格搜索。</p>
<p>另外，参数优化的迭代次数也要适中，太低可能还未达到最优，太高浪费时间。一个简单的办法是，在开始时设置一个非常大的迭代次数，但是当梯度向量的值变得很微小时中断算法——也就是当它的范数变得低于（称为容差）时，因为这时梯度下降已经（几乎）到达了最小值。</p>
<p><em><strong>随机梯度下降（SGD）：</strong></em></p>
<p>批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅<u>基于该单个实例来计算梯度</u>。这让算法变得快多了，也可以被用来训练海量的数据集。</p>
<p>另一方面，由于算法的随机性质，它比批量梯度下降要不规则得多。成本函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。但是即使它到达了最小值，依旧还会持续反弹，永远不会停止。所以算法停下来的参数值肯定是足够好的，但不是最优的。</p>
<p>随机梯度下降其实可以<u>帮助算法跳出局部最小值</u>，所以相比批量梯度下降，它对找到全局最小值更有优势。因此，随机性的好处在于可以逃离局部最优，但缺点是<u>永远定位不出最小值</u>。要解决这个困境，有一个办法是<u>逐步降低学习率</u>（模拟退火）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机梯度下降，n_iter为迭代次数，penalty为惩罚项（正则化），eta0为迭代次数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(n_iter=<span class="number">50</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>)</span><br><span class="line">sgd_reg.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(sgd_reg.intercept_, sgd_reg.coef_)</span><br></pre></td></tr></table></figure>

<p><strong>小批量梯度下降：</strong></p>
<p>每一步的梯度计算，基于一小部分随机的实例集也就是小批量。相比随机梯度下降，小批量梯度下降的主要优势在于可以从矩阵运算的硬件优化中获得显著的性能提升，特别是需要用到图形处理器时。</p>
<p>这个算法在参数空间层面的前进过程也不像SGD那样不稳定，特别是批量较大时。所以小批量梯度下降最终会比SGD更接近最小值一些。但是另一方面，它可能更难从局部最小值中逃脱</p>
<h4 id="3-多项式回归"><a href="#3-多项式回归" class="headerlink" title="3. 多项式回归"></a>3. 多项式回归</h4><p>当数据集并非线性时，可以将每个特征的幂次方添加为一个新特征，在拓展过的特征集上训练线性模型。这种方法被称为多项式回归。</p>
<p>PolynomialFeatures会在给定的多项式阶数下，添加所有特征组合。例如，有两个特征a和b，阶数degree&#x3D;3，PolynomialFeatures不只会添加特征a2、a3、b2和b3，还会添加组合ab、a2b以及ab2。</p>
<p>所以要小心特征组合的数量爆炸。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多项式回归，增加原特征的平方作为新特征</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line"><span class="comment"># X_poly现在包含原本的特征X和该特征的平方</span></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br><span class="line"><span class="built_in">print</span>(lin_reg.intercept_, lin_reg.coef_)</span><br></pre></td></tr></table></figure>

<p>或者使用管道技术一次完成：、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([(<span class="string">&quot;poly_features&quot;</span>, PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)), (<span class="string">&quot;sgd_reg&quot;</span>, LinearRegression())])</span><br><span class="line">polynomial_regression.fit(X, y)</span><br></pre></td></tr></table></figure>

<h4 id="4-学习曲线"><a href="#4-学习曲线" class="headerlink" title="4. 学习曲线"></a>4. 学习曲线</h4><p>模型训练时会出现欠拟合和过拟合的问题。</p>
<p>如果无论怎么增大数据集，训练集和测试集的误差接近且较高，那么模型可能欠拟合。这时增大数据集是无效的，需要使用更复杂的模型和寻找更好的特征。</p>
<p>如果训练集和测试集的误差有一定差距，且在训练集上模型的表现好很多，那么说明模型过拟合了。这时需要增加训练数据，直到测试误差接近训练误差。或者对模型进行正则化。</p>
<h4 id="5-正则线性模型"><a href="#5-正则线性模型" class="headerlink" title="5. 正则线性模型"></a>5. 正则线性模型</h4><p>减少过度拟合的一个好办法就是对模型正则化（即约束它）：它拥有的自由度越低，就越不容易过度拟合数据。</p>
<p>比如对于多项式模型来说，正则化可以是降低多项式的阶数。对于线性模型，正则化通常通过约束模型的权重来实现。</p>
<p><u><em><strong>岭回归（Ridge Regression）：</strong></em></u></p>
<p>在成本函数中添加一个正则项，使得学习中的算法不仅需要拟合数据，同时还要让模型权重保持最小。</p>
<p>在执行岭回归之前，必须对数据进行缩放（例如使用StandardScaler），因为它对输入特征的大小非常敏感。大多数正则化模型都是如此。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427164855903.png" alt="image-20210427164855903" style="zoom:50%;">

<p>超参数α控制的是对模型进行正则化的程度。如果α&#x3D;0，则岭回归就是线性模型。如果α非常大，那么所有的权重都将非常接近于零，结果是一条穿过数据平均值的水平线。</p>
<p>注意，正则项<u>只能在训练的时候添加到成本函数中</u>，一旦训练完成，你需要使用未经正则化的性能指标来评估模型性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用标准方程求解的岭回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge_reg = Ridge(alpha=<span class="number">1</span>, solver=<span class="string">&quot;cholesky&quot;</span>)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用梯度下降的岭回归,正则项为l2表示岭回归</span></span><br><span class="line">sgd_reg = SGDRegressor(penalty=<span class="string">&quot;l2&quot;</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line"><span class="built_in">print</span>(sgd_reg.predict([[<span class="number">1.5</span>]]))</span><br></pre></td></tr></table></figure>

<p>超参数penalty设置的是使用正则项的类型。设为”l2”表示希望SGD在成本函数中添加一个正则项，等于权重向量的l2范数的平方的一半，即岭回归。</p>
<p><u><em><strong>套索回归（Lasso Regression）：</strong></em></u></p>
<p>与岭回归一样，它也是向成本函数增加一个正则项，但是它增加的是权重向量的l1范数，而不是l2范数的平方的一半。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427183920256.png" alt="image-20210427183920256" style="zoom: 33%;">

<p>Lasso回归的一个重要特点是它倾向于完全消除掉最不重要特征的权重（也就是将它们设置为零）。也就是说，Lasso回归会自动执行特征选择并输出一个稀疏模型（即只有很少的特征有非零权重）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Lasso回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">lasso_reg = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line">lasso_reg.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(lasso_reg.predict([[<span class="number">1.5</span>]]))</span><br></pre></td></tr></table></figure>

<p><u><em><strong>弹性网络（Elastic Net）：</strong></em></u></p>
<p>弹性网络是岭回归与Lasso回归之间的中间地带。其正则项就是岭回归和Lasso回归的正则项的混合，混合比例通过r来控制。当r＝0时，弹性网络即等同于岭回归，而当r＝1时，即相当于Lasso回归。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427204356724.png" alt="image-20210427204356724" style="zoom:50%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用弹性网络</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line">elastic_net = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(elastic_net.predict([[<span class="number">1.5</span>]]))</span><br></pre></td></tr></table></figure>

<p><u><em><strong>如何选择正则化模型：</strong></em></u></p>
<p>模型正则化比不正则化可取。大多数情况下，应该避免使用纯线性回归。</p>
<p>岭回归是个不错的默认选择，但是如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso回归或是弹性网络，因为它们会将无用特征的权重降为零。</p>
<p>一般而言，弹性网络优于Lasso回归，因为当特征数量超过训练实例数量，又或者是几个特征强相关时，Lasso回归的表现可能非常不稳定。</p>
<h4 id="6-逻辑回归（分类）"><a href="#6-逻辑回归（分类）" class="headerlink" title="6. 逻辑回归（分类）"></a>6. 逻辑回归（分类）</h4><p>将数据正则化之后，比如使用StandardScaler类，使用LogisticRegression类进行模型建立。逻辑回归被广泛用于估算一个实例属于某个特定类别的概率。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427232532919.png" alt="image-20210427232532919"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">log_reg = LogisticRegression(C=<span class="number">1e5</span>)</span><br><span class="line">log_reg.fit(x_train, y_train)</span><br><span class="line">pre_pro = log_reg.predict_proba(x_test)</span><br></pre></td></tr></table></figure>

<p>如果你要求它预测出类别（使用predict（）方法而不是predict_proba（）方法），它将返回一个可能性最大的类别。</p>
<p>与其他线性模型一样，逻辑回归模型可以用l1或l2惩罚函数来正则化。Scikit-Learn默认添加的是l2函数。</p>
<p>控制Scikit-Learn的LogisticRegression模型正则化程度的超参数不是alpha（其他线性模型使用alpha），而是它的逆反：C，<u>C的值越高，模型正则化程度越高。</u></p>
<p><u><em><strong>Softmax回归:</strong></em></u></p>
<p>逻辑回归模型经过推广，可以直接支持多个类别，而不需要训练并组合多个二元分类器（如第3章所述）。这就是Softmax回归。</p>
<p>原理很简单：对于一个给定的实例x，Softmax回归模型首先计算出每个类别k的分数sk（x），然后对这些分数应用softmax函数（也叫归一化指数），估算出每个类别的概率。跟逻辑回归分类器一样，Softmax回归分类器将估算概率值最高的类别作为预测类别（也就是分数最高的类别）。</p>
<p>Softmax回归分类器一次只会预测一个类别（也就是说，它是多类别，但是不是多输出），所以它应该仅适用于互斥的类别之上。</p>
<p>Softmax回归的训练目标是得到一个能对目标类别做出高概率估算的模型（也就是其他类别的概率相应要很低），其成本函数（交叉熵）如下：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427232025182.png" alt="image-20210427232025182" style="zoom:67%;">

<p>当只有两个类别（K&#x3D;2）时，该成本函数等价于逻辑回归的成本函数（log损失函数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg = LogisticRegression(multi_class=<span class="string">&quot;multinomial&quot;</span>,solver=<span class="string">&quot;lbfgs&quot;</span>, C=<span class="number">10</span>)</span><br><span class="line">softmax_reg.fit(X, y)</span><br><span class="line">softmax_reg.predict([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="第二章-KNN算法"><a href="#第二章-KNN算法" class="headerlink" title="第二章 KNN算法"></a>第二章 KNN算法</h2><p><strong>优点</strong>：精度高，对异常值不敏感，无数据输入假定。</p>
<p><strong>缺点</strong>：计算复杂度高、空间复杂度高。</p>
<p><strong>适用范围</strong>：数值型和标称型。</p>
<p>对未知类别属性的数据集中的每个点依次执行以下操作：<br>(1)  计算已知类别数据集中的点与当前点之间的距离（欧氏距离）；<br>(2)  按照距离递增次序排序；<br>(3)  选取与当前点距离最小的<strong>k</strong>个点；<br>(4)  确定前<strong>k</strong>个点所在类别的出现频率；<br>(5)  返回前<strong>k</strong>个点出现频率最高的类别作为当前点的预测分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 6折交叉验证</span></span><br><span class="line">scores = cross_val_score(knn, x, y, cv=<span class="number">6</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>当然，KNN也可以用来处理回归任务。返回邻近的K个样本点的标签值的平均数作为预测值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理回归任务</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">knn_clf = KNeighborsRegressor(n_neighbors=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h2 id="第三章-SVM算法"><a href="#第三章-SVM算法" class="headerlink" title="第三章 SVM算法"></a>第三章 SVM算法</h2><p>能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一，特别适用于中小型复杂数据集的分类。</p>
<p><u><em><strong>优点：</strong></em></u></p>
<ol>
<li>用于二元和多元分类器、回归和新奇性检测</li>
<li>良好的预测生成器，提供了鲁棒的过拟合、噪声数据和异常点处理</li>
<li>当变量比样本还多是依旧有效</li>
<li>快速，即使样本量大于1万</li>
<li>自动检测数据的非线性，不用做变量变换</li>
</ol>
<p><u><em><strong>缺点：</strong></em></u></p>
<ol>
<li>应用在二元分类表现最好，其他预测问题表现不是太好</li>
<li>变量比样例多很多的时候，有效性降低，需要使用其他方案，例如SGD方案</li>
<li>只提供预测结果，如果想要获取预测概率，需要额外方法去获取</li>
<li>如果想要最优结果，需要调参。</li>
</ol>
<p><u><em><strong>使用SVM预测模型的通用步骤：</strong></em></u></p>
<ol>
<li>选择使用的SVM类</li>
<li>用数据训练模型</li>
<li>检查验证误差并作为基准线</li>
<li>为SVM参数尝试不同的值</li>
<li>检查验证误差是否改进</li>
<li>再次使用最优参数的数据来训练模型</li>
</ol>
<h4 id="1-线性SVM分类"><a href="#1-线性SVM分类" class="headerlink" title="1. 线性SVM分类"></a>1. 线性SVM分类</h4><p>生成决策边界（实线所示），不仅分离类别，并且尽可能远离最近的训练实例（大间隔分类）。决策边界由最接近边界的训练实例确定（支持），这些实例被称为支持向量（下图中已圈出）。</p>
<p>可以将SVM分类器视为在类别之间拟合可能的最宽的”街道“（平行的虚线所示）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210423214002939.png" alt="image-20210423214002939"></p>
<p>SVM对<strong>特征缩放</strong>非常敏感，在垂直刻度和水平刻度上生成的决策边界可能存在很大的差异。在左图中，垂直刻度（0<del>90）比水平刻度（0</del>6）大得多，因此可能的决策边界接近于水平。在特征缩放（例如使用Scikit-Learn的StandardScaler）后，决策边界看起来好很多（见右图）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210423214542324.png" alt="image-20210423214542324"></p>
<p><u><em><strong>软间隔分类：</strong></em></u></p>
<p>如果严格地让所有实例都不在”街道“上，并且位于正确的一边，这就是硬间隔分类。硬间隔分类有2个问题：一个是它只在数据线性可分离时才有效；一个是对异常值特别敏感（会影响泛化能力）。</p>
<p>为了避免以上问题，灵活地使用模型，我们尽可能在保持街道宽阔和限制间隔违例（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是软间隔分类。在Scikit-Learn的SVM类中，可以通过超参数C来控制这个平衡：C值越小，则街道越宽，但是间隔违例也会越多。</p>
<p>如果你的SVM模型过度拟合，可以试试通过降低C来进行正则化。</p>
<p><u><em><strong>SMO算法：</strong></em></u></p>
<p>在求解SVM的决策边界（求解参数）时，会遇到规模正比于训练样本数量的问题，为了避开这个障碍，使用SMO算法来求解其中的参数。</p>
<p>SMO的思想：</p>
<p>每次选择2个变量，然后固定其他变量（参数），然后优化选择的这2个变量，因为每次只优化2个变量，所以非常高效。</p>
<p><u><em><strong>使用：</strong></em></u></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/20181206003046683.png" alt="img"></p>
<p><strong>LinearSVC类</strong>适用于<u>样本数量较多</u>的二元和多元分类（大于10000），它会对偏置项进行正则化，所以你需要先减去平均值，使训练集集中（归一化）。如果使用StandardScaler会自动进行这一步。此外，请确保超参数loss设置为”hinge”，因为它不是默认值。最后，为了获得更好的性能，还应该将超参数dual设置为False，除非特征数量比训练实例还多。可以使用管道技术将归一化和实例化算法统一起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">svm_clf = LinearSVC(C=<span class="number">1</span>, loss=<span class="string">&quot;hinge&quot;</span>, dual=<span class="literal">False</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># C越大，越接近硬间隔；使用hinge损失函数，随机种子为42</span></span><br><span class="line"><span class="comment"># svm_clf = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;linear_svc&quot;, LinearSVC(C=1, loss=&quot;hinge&quot;, random_state=42)), ])</span></span><br></pre></td></tr></table></figure>

<p><strong>SVC类</strong>可以使用核函数（后面会讲到），适用于<u>样本数量较少</u>的二元和多元分类（少于10000）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">svm_clf = SVC(kernel=<span class="string">&quot;linear&quot;</span>, C=<span class="number">1</span>)   <span class="comment"># 使用线性核函数</span></span><br></pre></td></tr></table></figure>

<p><strong>SGDClassifier类</strong>它不会像LinearSVC类那样快速收敛，但是对于内存处理不了的大型数据集（核外训练）或是在线分类任务，它非常有效。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="comment"># SGD模型，使用hinge损失函数，alpha为正则化项参数</span></span><br><span class="line">svm_clf = SGDClassifier(loss=<span class="string">&quot;hinge&quot;</span>, alpha=<span class="number">1</span>/(m*C))</span><br></pre></td></tr></table></figure>

<h4 id="2-非线性SVM分类"><a href="#2-非线性SVM分类" class="headerlink" title="2. 非线性SVM分类"></a>2. 非线性SVM分类</h4><p>有些情况下，数据集无法直接做到线性可分，解决方法之一就是添加更多的特征，将原始数据映射到更高维的空间，使其变得线性可分。</p>
<p><u><em><strong>核函数：</strong></em></u></p>
<p>添加特征会使得在高维空间计算样本特征内积变得困难，为了避开这个障碍，可以设想一个函数，使样本在高维特征空间的内积等于它们在原始样本空间中通过该函数计算的结果。使用该函数的就可以替代在高维甚至无穷维特征空间中的内积。这个函数就叫做”核函数“。</p>
<p><strong>线性核：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm_clf = SVC(kernel=<span class="string">&quot;linear&quot;</span>, C=<span class="number">1</span>)    <span class="comment"># SVC模型，使用线性核函数</span></span><br></pre></td></tr></table></figure>

<p><strong>多项式核：</strong></p>
<p>为了将数据集映射到高维特征空间，可以使用PolynomialFeatures转换器直接添加多项式特征。但问题是，如果多项式太低阶，处理不了非常复杂的数据集，而高阶则会创造出大量的特征，导致模型变得太慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">polynomial_svm_clf = Pipeline([(<span class="string">&quot;poly_features&quot;</span>, PolynomialFeatures(degree=<span class="number">3</span>)), (<span class="string">&quot;scaler&quot;</span>, StandardScaler()), (<span class="string">&quot;svm_clf&quot;</span>, LinearSVC(C=<span class="number">10</span>, loss=<span class="string">&quot;hinge&quot;</span>, random_state=<span class="number">42</span>))])</span><br></pre></td></tr></table></figure>

<p>为了解决这个问题，出现了多项式核函数。下面这段代码使用了一个3阶多项式内核训练SVM分类器。超参数coef0控制的是模型受高阶多项式还是低阶多项式影响的程度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">poly_kernel_svm_clf = Pipeline([(<span class="string">&quot;scaler&quot;</span>, StandardScaler()), (<span class="string">&quot;svm_clf&quot;</span>, SVC(kernel=<span class="string">&quot;poly&quot;</span>, degree=<span class="number">3</span>, coef0=<span class="number">1</span>, C=<span class="number">5</span>))])</span><br><span class="line">poly_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>

<p><strong>高斯核：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用RBF高斯核函数</span></span><br><span class="line">rbf_kernel_svm_clf = Pipeline([(<span class="string">&quot;scaler&quot;</span>, StandardScaler()),(<span class="string">&quot;svm_clf&quot;</span>, SVC(kernel=<span class="string">&quot;rbf&quot;</span>, gamma=<span class="number">5</span>, C=<span class="number">0.001</span>))])</span><br><span class="line">rbf_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>另外，还有拉普拉斯核、Sigmoid核。</p>
<p><strong><u><em>如何选择核函数：</em></u></strong></p>
<p>有一个经验法则是，永远先<u>从线性核函数开始</u>尝试（要记住，LinearSVC比SVC（kernel&#x3D;”linear”）快得多），特别是训练集非常大或特征非常多的时候。如果训练集不太大，你可以试试<u>高斯RBF核</u>，大多数情况下它都非常好用。如果你还有多余的时间和计算能力，你可以使用<u>交叉验证和网格搜索</u>来尝试一些其他的核函数，特别是那些专门针对你的数据集数据结构的核函数。</p>
<h4 id="3-SVM回归-SVR"><a href="#3-SVM回归-SVR" class="headerlink" title="3. SVM回归(SVR)"></a>3. SVM回归(SVR)</h4><p>SVM算法非常全面：它不仅支持线性和非线性分类，而且还支持线性和非线性回归。SVM回归要做的是让尽可能多的实例位于街道上，同时限制间隔违例（也就是不在街道上的实例）。</p>
<p>街道的宽度由超参数ε控制。在间隔内添加更多的实例不会影响模型的预测，所以这个模型被<br>称为ε不敏感。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210427095934141.png" alt="image-20210427095934141" style="zoom: 80%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVM回归</span></span><br><span class="line">svm_reg = LinearSVR(epsilon=<span class="number">1.5</span>)</span><br><span class="line">svm_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>要解决非线性回归任务，可以使用核化的SVM模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用核化的SVM模型进行回归</span></span><br><span class="line">svm_poly_reg = SVR(kernel=<span class="string">&quot;poly&quot;</span>, degree=<span class="number">2</span>, C=<span class="number">100</span>, epsilon=<span class="number">0.1</span>)</span><br><span class="line">svm_poly_reg.fit(X, y)</span><br></pre></td></tr></table></figure>



<h2 id="第四章-决策树DT"><a href="#第四章-决策树DT" class="headerlink" title="第四章 决策树DT"></a>第四章 决策树DT</h2><p>决策树是一种由结点和有向边构成的树形结构，结点类型分为内部结点和叶结点，每个内部结点代表对象的一个特征，叶结点则代表对象的类别。决策树中，每一个深度就是一次根据某一特征做出的判断。决策树的特质之一就是它们需要的数据准备工作非常少。特别是，完全不需要进行特征缩放或集中。</p>
<p>鸢尾花决策树：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210430093007719.png" alt="image-20210430093007719" style="zoom: 80%;">

<p>节点的samples属性统计它应用的训练实例数量（满足该节点属性的实例数量），value属性说明了该节点上每个类别的训练实例数量，gini属性衡量其不纯度（impurity，基尼不纯度）：如果应用的所有训练实例都属于同一个类别，那么节点就是“纯”的（gini&#x3D;0）。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210423150650767.png" alt="image-20210423150650767" style="zoom: 50%;">

<p>$P_{i,k}$是第$i$个节点上，类别为$k$的训练实例占比。</p>
<p>Scikit-Learn使用的是CART算法（Classification And Regression Tree），该算法仅生成二叉树，可用于分类和回归，使用：基尼不纯度来划分属性。但是，其他算法，比如ID3生成的决策树，其节点可以拥有两个以上的子节点，使用信息增益来划分属性。</p>
<h3 id="一、CART算法"><a href="#一、CART算法" class="headerlink" title="一、CART算法"></a>一、CART算法</h3><h4 id="1-CART算法（分类）"><a href="#1-CART算法（分类）" class="headerlink" title="1. CART算法（分类）"></a>1. CART算法（分类）</h4><p><strong><u><em>过程如下：</em></u></strong></p>
<p>（1）使用单个特征k和阈值tk（例如，花瓣长度≤2.45厘米）将训练集分成两个子集。k和tk就是使得成本函数$J$最小化（或者信息增益最大化）的$(k,t_k)$，成本函数衡量划分后的子集的不纯度。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210423150441237.png" alt="image-20210423150441237" style="zoom:50%;">

<p>（2）一旦成功将训练集一分为二，它将使用相同的逻辑，继续分裂子集，然后是子集的子集，依次循环递进。</p>
<p>（3）抵达最大深度（由超参数max_depth控制），或是再也找不到能够降低不纯度的分裂时，停止。</p>
<p>明显，决策树的思想是一种贪心选择，它并不会检视一次分裂的不纯度是否为可能的最低值，这样通常可以产生一个相当不错的解，但是不能保证是最优解。而寻找最优树是一个NP完全问题，即使训练集很小时间复杂度也很高，很难解决，所以我们必须接受这个”相当不错“的解。</p>
<p><em><strong><u>计算复杂度：</u></strong></em></p>
<p>决策树总体预测复杂度是O(log2(m))，m为实例数量，可以看出复杂度与特征数量无关，所以即便是处理大型数据集，预测也很快。</p>
<p>但是，训练时在每一个节点，算法都需要在所有样本上比较所有特征（如果设置了划分时考虑的最大特征数<strong>max_features</strong>会少一些）。这导致训练的复杂度为O(n×m log(m))，。对于小型训练集（几千个实例以内），Scikit-Learn可以通过对数据预处理（设置<strong>presort&#x3D;True</strong>表示对样本进行预排序）来加快训练，但是对于较大训练集而言，可能会减慢训练的速度。</p>
<p><strong><u><em>过拟合与正则化超参数：</em></u></strong></p>
<p>决策树在训练时不会确定参数的数量（树的深度不确定），也叫非参数模型，这会导致模型结构自由而紧密地贴近数据，很可能过拟合。而比如线性回归有预先设定好一部分参数，所以其自由度受限，降低过拟合的风险（但是相应的也增加了拟合不足的风险）。</p>
<p>为了避免过拟合，需要在训练中降低决策树的自由度，即正则化。正则化超参数的选择取决于使用的模型，但是通常至少可以限制决策树的深度。在Scikit-Learn中，这由超参数<strong>max_depth</strong>控制（默认值为None，意味着无限制）。减小max_depth可使模型正则化，从而降低过度拟合的风险。另外，DecisionTreeClassifier类还有一些其他的参数也可以限制决策树的形状，比如<strong>min_samples_split</strong>（分裂前节点必须有的最小样本<br>数），<strong>min_samples_leaf</strong>（叶节点必须有的最小样本数量）等。</p>
<p>还可以先不加约束地训练模型，然后再对不必要的节点进行剪枝（删除），比如一个节点的子节点全部为叶节点，则该节点可被认为不必要，删除；比如χ2测试，是用来估算“提升纯粹是出于偶然”（被称为虚假设）的概率。如果这个概率（称之为p值）高于一个给定阈值（通常是5%，由超参数控制），那么这个节点可被认为不必要，其子节点可被删除。</p>
<h4 id="2-CART算法（回归）"><a href="#2-CART算法（回归）" class="headerlink" title="2. CART算法（回归）"></a>2. CART算法（回归）</h4><p>决策树也可以用来完成回归任务，用Scikit_Learn的DecisionTreeRegressor来构建一个回归树。</p>
<p>与分类决策树的主要差别在于，每个节点上不再是预测一个类别而是预测一个值。预测结果就是与最后到达的叶节点关联的110个实例的平均目标值。MSE表示在这个叶节点上得到的预测结果的均方误差。</p>
<p>回归任务中，CART算法的工作原理跟前面介绍的大致相同，唯一不同在于，它分裂训练集的方式不是最小化不纯度，而是最小化MSE。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210423192435313.png" alt="image-20210423192435313"></p>
<p>同样，用于回归的决策树也会有过拟合的可能，所以需要设置min_samples_leaf。</p>
<h4 id="3-优缺点"><a href="#3-优缺点" class="headerlink" title="3. 优缺点"></a>3. 优缺点</h4><p>决策树使用简单，不受特征数量的限制，但是青睐正交的决策边界（所有的分裂都与轴线垂直），这导致它们对训练集的旋转非常敏感，可能导致泛化不佳，限制这种问题的方法之一是使用PCA，让训练数据定位在一个更好的方向上。更概括地说，决策树的主要问题是它们对训练数据中的小变化非常敏感。</p>
<h3 id="二、ID3算法"><a href="#二、ID3算法" class="headerlink" title="二、ID3算法"></a>二、ID3算法</h3><h3 id="三、C4-5算法（J48）"><a href="#三、C4-5算法（J48）" class="headerlink" title="三、C4.5算法（J48）"></a>三、C4.5算法（J48）</h3><p>使用信息增益选择特征</p>
<h2 id="第五章-集成学习与随机森林"><a href="#第五章-集成学习与随机森林" class="headerlink" title="第五章 集成学习与随机森林"></a>第五章 集成学习与随机森林</h2><h3 id="一、模型融合"><a href="#一、模型融合" class="headerlink" title="一、模型融合"></a>一、模型融合</h3><h4 id="1-投票分类器"><a href="#1-投票分类器" class="headerlink" title="1. 投票分类器"></a>1. 投票分类器</h4><p>不同的算法在相同训练集上进行训练，得到多个预测模型，然后基于多个模型的预测结果投票选出最终结果。</p>
<p><strong><u><em>硬投票法：</em></u></strong></p>
<p>直接让各个预测模型给出预测结果（投票），然后选择大多数模型投票的类别作为最终预测结果。</p>
<p><strong><u><em>软投票法：</em></u></strong></p>
<p>模型估算出类别的概率（predict_proba()），将概率在所有单个分类器上（加权）平均，选出平均概率最高的类别进行预测。软投票法比硬投票的表现更优，因为它基于哪些高度自信的投票更高的权重。</p>
<h4 id="2-bagging-和-pasting"><a href="#2-bagging-和-pasting" class="headerlink" title="2. bagging 和 pasting"></a>2. bagging 和 pasting</h4><p>对训练集随机采样，使用相同的算法在不同的训练子集上进行训练。采样时如果<u>将样本放回</u>，这种方法叫<u>bagging</u>，<u>不放回叫pasting</u>。（bootstrap&#x3D;True表示bagging，否则表示pasting）</p>
<p>一旦预测器训练完成，集成就可以通过简单地聚合所有预测器的预测，来对新实例做出预测。聚合函数通常是统计法（即最多数的预测好比硬投票分类器一样）用于分类，或是平均法用于回归。最终结果是，与直接在原始训练集上训练的单个预测器相比，集成方法的单个预测器的偏差更大，但是最终结果的偏差相近，方差更低。</p>
<p><strong><u><em>包外评估：</em></u></strong></p>
<p>使用bagging时，有些样本可能会被多次采样，有些样本可能不会被采样。不划分单独的测试集，而直接将那些未被采样的样本作为测试集，就是包外评估。</p>
<h4 id="3-stacking"><a href="#3-stacking" class="headerlink" title="3. stacking"></a>3. stacking</h4><p>第一层：将数据分为训练集和测试集，训练集再分为2个子集。首先在子集1上训练不同的模型：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428160136685.png" alt="image-20210428160136685" style="zoom: 67%;">

<p>第二层：分别使用前面训练好的几个模型对子集2进行预测，得到多个预测值。接着，使用这些预测值作为输入特征，创建一个新的训练集，并保留真实标签。在这个新的训练集上训练混合器，让它学习根据第一层的预测来在测试集上预测目标值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428160307753.png" alt="image-20210428160307753"></p>
<p>当然，这个模型还可以增加层数，增加的层都是使用上一层的预测值作为输入特征来训练模型。</p>
<p>比如下面是一个三层的stacking模型：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428160815773.png" alt="image-20210428160815773"></p>
<p>将训练集分为3个子集。第一层使用子集1进行模型的训练，得到3个模型。第二层中，使用第一层的模型在子集2上的预测作为输入特征来训练三个新的模型。第三层中，使用第二层的模型在子集3上的预测作为输入特征来训练最终模型。最后使用最终模型在测试集上进行预测。</p>
<p>不幸的是，Scikit-Learn不直接支持堆叠，但是自己堆出stacking的实现并不太难。或者也可以使用开源的实现方案，例如brew（可从<a target="_blank" rel="noopener" href="https://github.com/viisar/brew%E8%8E%B7%E5%BE%97%EF%BC%89%E3%80%82">https://github.com/viisar/brew获得）。</a></p>
<h3 id="二、随机森林RF"><a href="#二、随机森林RF" class="headerlink" title="二、随机森林RF"></a>二、随机森林RF</h3><p>随机森林就是决策树的集成，通常采用bagging集成方法（有时也可以是pasting）。</p>
<p>训练子集的大小通过max_samples来设置。在sklearn中，除了先构建一个BaggingClassifier然后将结果传输到DecisionTreeClassifier，还有一种方法就是直接使用RandomForestClassifier或者RandomForestRegressor类。</p>
<p>随机森林在树的生长上引入了更多的随机性：分裂节点时不再是搜索最好的特征，而是在一个随机生成的特征子集里搜索最好的特征。这导致决策树具有更大的多样性，用更高的偏差换取更低的方差，总之，还是产生了一个整体性能更优的模型。</p>
<h3 id="三、提升法boosting"><a href="#三、提升法boosting" class="headerlink" title="三、提升法boosting"></a>三、提升法boosting</h3><p>提升法（Boosting，最初被称为假设提升）是指可以将几个弱学习器结合成一个强学习器的任意集成方法。大多数提升法的总体思路是<u>循环训练预测器</u>，每一次都对其前序做出一些改正。</p>
<h4 id="1-AdaBoost"><a href="#1-AdaBoost" class="headerlink" title="1. AdaBoost"></a>1. AdaBoost</h4><p>新预测器对其前序进行纠正的办法之一，就是更多地关注前序拟合不足的训练实例。从而使新的预测器不断地越来越专注于难缠的问题，这就是AdaBoost使用的技术。</p>
<p><u><em><strong>过程如下：</strong></em></u></p>
<p>（1）训练一个基础分类器（比如决策树），用它对训练集进行预测。</p>
<p>（2）然后对错误分类的训练实例增加其相对权重</p>
<p>（3）使用这个最新的权重对第二个分类器进行训练，然后再次对训练集进行预测</p>
<p>（4）继续对错误分类的训练实例增加其权重，如此循环</p>
<p>（5）当到达所需数量的预测器，或得到完美的预测器时，算法停止</p>
<p>（6）这样就得到若干个预测器，再使用bagging或pasting等集成方法得到最终预测结果</p>
<p><u><em><strong>优缺点：</strong></em></u></p>
<p>AdaBoost不再是调整单个预测器的参数使损失函数最小化，而是不断在集成中加入预测器，使模型越来越好。而且每次训练是基于加权的训练集。</p>
<p>这种依序学习技术有一个重要的缺陷就是无法并行（哪怕只是一部分），因为每个预测器只能在前一个预测器训练完成并评估之后才能开始训练，在这一点上AdaBoost的表现不及bagging和pasting方法。</p>
<p><u><em><strong>权重：</strong></em></u></p>
<p>AdaBoost中，每个预测器有一个权重，通过其加权误差率、学习率计算而来，预测器的准确率越高，其权重就越高。同时，每个样本实例也有权重。最开始每个实例的权重都一样，一个预测器预测完成后，会对实例的权重进行更新，也就是提升被错误分类的实例的权重。</p>
<p><u><em><strong>使用：</strong></em></u></p>
<p>Scikit-Learn使用的其实是AdaBoost的一个多分类版本，叫作SAMME（<a target="_blank" rel="noopener" href="http://goo.gl/Eji2vR%EF%BC%89[4]%EF%BC%88%E5%9F%BA%E4%BA%8E%E5%A4%9A%E7%B1%BB%E6%8C%87%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%90%E6%AD%A5%E6%B7%BB%E5%8A%A0%E6%A8%A1%E5%9E%8B%EF%BC%89%E3%80%82%E5%BD%93%E5%8F%AA%E6%9C%89%E4%B8%A4%E4%B8%AA%E7%B1%BB%E5%88%AB%E6%97%B6%EF%BC%8CSAMME%E5%8D%B3%E7%AD%89%E5%90%8C%E4%BA%8EAdaBoost%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E5%A6%82%E6%9E%9C%E9%A2%84%E6%B5%8B%E5%99%A8%E5%8F%AF%E4%BB%A5%E4%BC%B0%E7%AE%97%E7%B1%BB%E5%88%AB%E6%A6%82%E7%8E%87%EF%BC%88%E5%8D%B3%E5%85%B7%E6%9C%89predict_proba%EF%BC%88%EF%BC%89%E6%96%B9%E6%B3%95%EF%BC%89%EF%BC%8CScikit-Learn%E4%BC%9A%E4%BD%BF%E7%94%A8%E4%B8%80%E7%A7%8DSAMME%E7%9A%84%E5%8F%98%E4%BD%93%EF%BC%8C%E7%A7%B0%E4%B8%BASAMME.R%EF%BC%88R%E4%BB%A3%E8%A1%A8">http://goo.gl/Eji2vR）[4]（基于多类指数损失函数的逐步添加模型）。当只有两个类别时，SAMME即等同于AdaBoost。此外，如果预测器可以估算类别概率（即具有predict_proba（）方法），Scikit-Learn会使用一种SAMME的变体，称为SAMME.R（R代表</a> “Real”,它依赖的是类别概率而不是类别预测，通常表现更好。</p>
<h4 id="2-梯度提升"><a href="#2-梯度提升" class="headerlink" title="2. 梯度提升"></a>2. 梯度提升</h4><p>GBDT</p>
<h3 id="四、XGBoost"><a href="#四、XGBoost" class="headerlink" title="四、XGBoost"></a>四、XGBoost</h3><h2 id="第六章-贝叶斯分类器"><a href="#第六章-贝叶斯分类器" class="headerlink" title="第六章 贝叶斯分类器"></a>第六章 贝叶斯分类器</h2><p>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法。</p>
<h3 id="一、贝叶斯公式："><a href="#一、贝叶斯公式：" class="headerlink" title="一、贝叶斯公式："></a>一、贝叶斯公式：</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428101924523.png" alt="image-20210428101924523"></p>
<p><u><em><strong>原理：</strong></em></u></p>
<p>X：特征向量；Y：类别<br><strong>先验概率</strong>P(X)：指根据以往经验和分析得到的概率。<br><strong>后验概率</strong>P(Y|X)：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。<br><strong>类条件概率</strong>P(X|Y)：在已知某类别的特征空间中，出现特征值X的概率密度。</p>
<p>如果需要选出某样本属于哪类，则需要根据该条样本求出它属于每个类的概率，选择最大概率的那个类作为分类结果。</p>
<p>由于结果的产生是比较属于各个类别的概率，所以计算的概率的分母都是P(X)，可以忽略掉P(X)。同时P(Y)容易求出，那么我们关注P(X|Y)。朴素贝叶斯之所以朴素是因为它<u>假设X的每个特征都是独立的</u>，回归原始。故而P(X|Y)的概率就可以计算为：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428102705885.png" alt="image-20210428102705885"></p>
<p>故，朴素贝叶斯公式：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428102729512.png" alt="image-20210428102729512"></p>
<h3 id="二、朴素贝叶斯模型："><a href="#二、朴素贝叶斯模型：" class="headerlink" title="二、朴素贝叶斯模型："></a>二、朴素贝叶斯模型：</h3><h4 id="1-多项式模型："><a href="#1-多项式模型：" class="headerlink" title="1. 多项式模型："></a>1. 多项式模型：</h4><p>多项式模型在计算先验概率P(Yk)和条件概率P(xi|Yk)时，会做一些平滑处理，具体公式为：<br>$$<br>P(Y_k) &#x3D; \frac{N_{Y_k}+\alpha}{N+K\alpha}<br>$$</p>
<p>$N_{Y_k}$：类别为$Y_k$的样本数</p>
<p>N：总样本数</p>
<p>K：总的类别个数</p>
<p>$\alpha$：平滑值</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428110956018.png" alt="image-20210428110956018"></p>
<p>$N_{Y_k,x_i}$：类别为$Y_k$，且特征为$x_i$的样本数</p>
<p>n：特征$x_i$可以选择的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多项式朴素贝叶斯，alpha为平滑参数，默认为1</span></span><br><span class="line"><span class="comment"># class_prior为类先验概率，若指定了该参数，就按指定的参数计算。class_log_prior_取值就是class_prior转换成log后的结果（防止下溢出）</span></span><br><span class="line"><span class="comment"># fit_prior默认为True，表示是否学习先验概率，为False时表示所有类标记具有相同的先验概率（等于类标记总个数N分之一）</span></span><br><span class="line">pnb_clf = MultinomialNB(alpha=<span class="number">2.0</span>, class_prior=<span class="literal">None</span>, fit_prior=<span class="literal">True</span>)</span><br><span class="line">pnb_clf.fit(X,y)</span><br><span class="line"><span class="built_in">print</span>(pnb_clf.predict([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>]]))</span><br></pre></td></tr></table></figure>

<h4 id="2-高斯模型："><a href="#2-高斯模型：" class="headerlink" title="2. 高斯模型："></a>2. 高斯模型：</h4><p>当特征是连续变量的时候，假设特征分布为正太分布，根据样本算出均值和方差，再求得概率。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428111749260.png" alt="image-20210428111749260"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高斯朴素贝叶斯</span></span><br><span class="line"><span class="comment"># 参数priors默认为None，指各个类标记对应的先验概率</span></span><br><span class="line">gnb_clf = GaussianNB()</span><br><span class="line">gnb_clf.fit(X,y)    <span class="comment"># partial_fit表示增量学习</span></span><br><span class="line"><span class="built_in">print</span>(gnb_clf.predict([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>]]))</span><br></pre></td></tr></table></figure>

<h4 id="3-伯努利模型："><a href="#3-伯努利模型：" class="headerlink" title="3. 伯努利模型："></a>3. 伯努利模型：</h4><p>伯努利模型适用于离散特征的情况，每个特征的取值只能是1和0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://frankcao3-picgo.oss-cn-shenzhen.aliyuncs.com/img/image-20210428111837792.png" alt="image-20210428111837792"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伯努利朴素贝叶斯（每个特征的取值只能是1和0）</span></span><br><span class="line"><span class="comment"># 参数binarize指将数据特征二值化的阈值</span></span><br><span class="line">bnb_clf = BernoulliNB(alpha=<span class="number">2.0</span>,binarize = <span class="number">3.0</span>,fit_prior=<span class="literal">True</span>)</span><br><span class="line">bnb_clf.fit(X,y)</span><br><span class="line"><span class="built_in">print</span>(bnb_clf.predict([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>]]))</span><br></pre></td></tr></table></figure>

<p><u><em><strong>算法流程：</strong></em></u></p>
<ol>
<li><p>处理数据，得到m个具有n个特征的样本，这些样本分别属于${Y_1,Y_2,Y_3}$类别。</p>
</li>
<li><p>通过数据分析可以得到每个特征的类条件概率$P(x_i|Y)$，再通过全概率公式求得$P(X)$。</p>
</li>
</ol>
<p>  $P(X)&#x3D;P(X|Y_1)P(Y_1)+P(X|Y_2)P(Y_2)+P(X|Y_3)P(Y_3)$</p>
<ol start="3">
<li><p>其中$P(X|Y_i)$可根据特征独立性展开。</p>
</li>
<li><p>将求得的先验概率和类条件概率带入朴素贝叶斯公式，求得每个类别的后验概率。我们可以选择概率最大的类别为最后确定的类别.</p>
</li>
</ol>
<h2 id="第七章-神经网络"><a href="#第七章-神经网络" class="headerlink" title="第七章 神经网络"></a>第七章 神经网络</h2><h2 id="第八章-聚类"><a href="#第八章-聚类" class="headerlink" title="第八章 聚类"></a>第八章 聚类</h2><p>K-means</p>
<h1 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h1><p>《机器学习实战》目录：</p>
<p>1~7为分类算法，8~9为回归算法，10~12为无监督算法</p>
<ol>
<li>ML基础</li>
<li>KNN</li>
<li>DT</li>
<li>NB（概率分布）</li>
<li>LR（算法优化，处理数据集合中的缺失值）</li>
<li>SVM</li>
<li>AdaBoost</li>
<li>回归、去噪、局部线性回归</li>
<li>CART（回归部分）</li>
<li>k-means</li>
<li>Apriori</li>
<li>FP-Growth</li>
<li><ol start="14">
<li>主成分分析、奇异值分解、MapReduce</li>
</ol>
</li>
</ol>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">ccb</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://frankcao3.github.io/posts/61497">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://frankcao3.github.io/posts/61497')">机器学习实战</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://frankcao3.github.io/posts/61497"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=机器学习实战&amp;url=https://frankcao3.github.io/posts/61497&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="rm.copyPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>机器学习<span class="tagsPageCount">1</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/9423"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">STL</div></div></a></div><div class="next-post pull-right"><a href="/posts/15275"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">文件读取</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98"><span class="toc-number">1.</span> <span class="toc-text">机器学习实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%88%E5%AF%BC%E7%9F%A5%E8%AF%86%EF%BC%9A"><span class="toc-number">1.1.</span> <span class="toc-text">先导知识：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">第一章 线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">1. 线性回归（回归）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">2. 梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.0.3.</span> <span class="toc-text">3. 多项式回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="toc-number">1.2.0.4.</span> <span class="toc-text">4. 学习曲线</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%AD%A3%E5%88%99%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.0.5.</span> <span class="toc-text">5. 正则线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89"><span class="toc-number">1.2.0.6.</span> <span class="toc-text">6. 逻辑回归（分类）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-KNN%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">第二章 KNN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-SVM%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">第三章 SVM算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">1. 线性SVM分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%9D%9E%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">2. 非线性SVM分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SVM%E5%9B%9E%E5%BD%92-SVR"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">3. SVM回归(SVR)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91DT"><span class="toc-number">1.5.</span> <span class="toc-text">第四章 决策树DT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81CART%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.1.</span> <span class="toc-text">一、CART算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-CART%E7%AE%97%E6%B3%95%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">1. CART算法（分类）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-CART%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">2. CART算法（回归）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.5.1.3.</span> <span class="toc-text">3. 优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81ID3%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.2.</span> <span class="toc-text">二、ID3算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81C4-5%E7%AE%97%E6%B3%95%EF%BC%88J48%EF%BC%89"><span class="toc-number">1.5.3.</span> <span class="toc-text">三、C4.5算法（J48）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.6.</span> <span class="toc-text">第五章 集成学习与随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88"><span class="toc-number">1.6.1.</span> <span class="toc-text">一、模型融合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%8A%95%E7%A5%A8%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">1. 投票分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-bagging-%E5%92%8C-pasting"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">2. bagging 和 pasting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-stacking"><span class="toc-number">1.6.1.3.</span> <span class="toc-text">3. stacking</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97RF"><span class="toc-number">1.6.2.</span> <span class="toc-text">二、随机森林RF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%8F%90%E5%8D%87%E6%B3%95boosting"><span class="toc-number">1.6.3.</span> <span class="toc-text">三、提升法boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-AdaBoost"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">1. AdaBoost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">2. 梯度提升</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81XGBoost"><span class="toc-number">1.6.4.</span> <span class="toc-text">四、XGBoost</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.7.</span> <span class="toc-text">第六章 贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%EF%BC%9A"><span class="toc-number">1.7.1.</span> <span class="toc-text">一、贝叶斯公式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">1.7.2.</span> <span class="toc-text">二、朴素贝叶斯模型：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">1. 多项式模型：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">2. 高斯模型：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">3. 伯努利模型：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.8.</span> <span class="toc-text">第七章 神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E8%81%9A%E7%B1%BB"><span class="toc-number">1.9.</span> <span class="toc-text">第八章 聚类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">附录：</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/16107" title="Hello World">Hello World</a><time datetime="2023-10-05T11:40:46.695Z" title="Created 2023-10-05 19:40:46">2023-10-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/34953" title="渗透测试全流程">渗透测试全流程</a><time datetime="2022-08-04T16:00:00.000Z" title="Created 2022-08-05 00:00:00">2022-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/39395" title="基础概念与信息收集">基础概念与信息收集</a><time datetime="2022-08-04T16:00:00.000Z" title="Created 2022-08-05 00:00:00">2022-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/58219" title="反序列化漏洞">反序列化漏洞</a><time datetime="2022-08-04T16:00:00.000Z" title="Created 2022-08-05 00:00:00">2022-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/57467" title="XXE">XXE</a><time datetime="2022-08-04T16:00:00.000Z" title="Created 2022-08-05 00:00:00">2022-08-05</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2023 By <a class="footer-bar-link" href="/" title="ccb" target="_blank">ccb</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">Articles</div><div class="length-num">40</div></a><a href="/tags/" title="tag"><div class="headline">Tags</div><div class="length-num">6</div></a><a href="/categories/" title="category"><div class="headline">Categories</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">Function</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="Display Mode"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>Display Mode</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/DVWA%E9%9D%B6%E5%9C%BA/" style="font-size: 0.88rem;">DVWA靶场<sup>10</sup></a><a href="/tags/%E5%AE%9E%E8%AE%AD-%E7%BD%91%E7%BB%9C%E5%BB%BA%E8%AE%BE%E9%83%A8%E5%88%86/" style="font-size: 0.88rem;">实训-网络建设部分<sup>7</sup></a><a href="/tags/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" style="font-size: 0.88rem;">搭建博客<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">机器学习<sup>1</sup></a><a href="/tags/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95/" style="font-size: 0.88rem;">渗透测试<sup>15</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 0.88rem;">计算机基础<sup>5</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.8",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 ccb 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>